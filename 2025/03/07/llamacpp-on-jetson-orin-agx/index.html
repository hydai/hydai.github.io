<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hyd.ai","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.21.1","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"找到 ${hits} 個搜索結果（用時 ${time} 毫秒）","hits":"找到 ${hits} 個搜索結果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="簡介由於工作的緣故，需要將 llama.cpp 作為 WASI-NN 的後端所使用，來讓 WebAssembly 能具備使用 AI 模型的能力，也因此需要在各種平台上編譯 llama.cpp 作為我們的相依性函式庫。而 NVIDIA Jetson Orin AGX 64GB 的版本，作為提供相對大的 VRAM 與支援 CUDA 的平台，自然是我們花許多力氣在上面進行測試與最佳化的目標。本文將詳">
<meta property="og:type" content="article">
<meta property="og:title" content="在 NVIDIA Jetson Orin AGX 上編譯 llama.cpp 與部署 AI 應用">
<meta property="og:url" content="https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/index.html">
<meta property="og:site_name" content="hydaiの空想世界">
<meta property="og:description" content="簡介由於工作的緣故，需要將 llama.cpp 作為 WASI-NN 的後端所使用，來讓 WebAssembly 能具備使用 AI 模型的能力，也因此需要在各種平台上編譯 llama.cpp 作為我們的相依性函式庫。而 NVIDIA Jetson Orin AGX 64GB 的版本，作為提供相對大的 VRAM 與支援 CUDA 的平台，自然是我們花許多力氣在上面進行測試與最佳化的目標。本文將詳">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/img.jpg">
<meta property="article:published_time" content="2025-03-07T05:28:00.000Z">
<meta property="article:modified_time" content="2025-03-07T07:31:15.968Z">
<meta property="article:author" content="hydai">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="NVIDIA">
<meta property="article:tag" content="Jetson">
<meta property="article:tag" content="Orin">
<meta property="article:tag" content="AGX">
<meta property="article:tag" content="JetPack">
<meta property="article:tag" content="CUDA">
<meta property="article:tag" content="GGUF">
<meta property="article:tag" content="llamacpp">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/img.jpg">


<link rel="canonical" href="https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-TW","comments":true,"permalink":"https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/","path":"2025/03/07/llamacpp-on-jetson-orin-agx/","title":"在 NVIDIA Jetson Orin AGX 上編譯 llama.cpp 與部署 AI 應用"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>在 NVIDIA Jetson Orin AGX 上編譯 llama.cpp 與部署 AI 應用 | hydaiの空想世界</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-78643171-1"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"UA-78643171-1","only_pageview":false,"measure_protocol_api_secret":null}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="hydaiの空想世界" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">hydaiの空想世界</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜尋" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首頁</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分類</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B0%A1%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">簡介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%87%E7%B4%9A%E8%87%B3-JetPack-6-2"><span class="nav-number">2.</span> <span class="nav-text">升級至 JetPack 6.2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B7%A8%E8%AD%AF-llama-cpp"><span class="nav-number">3.</span> <span class="nav-text">編譯 llama.cpp</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%9D%E7%9B%B8%E4%BE%9D%E6%80%A7%E5%A5%97%E4%BB%B6"><span class="nav-number">3.1.</span> <span class="nav-text">安裝相依性套件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BC%89-llama-cpp"><span class="nav-number">3.2.</span> <span class="nav-text">下載 llama.cpp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B7%A8%E8%AD%AF-llama-cpp-1"><span class="nav-number">3.3.</span> <span class="nav-text">編譯 llama.cpp</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%8B%A5%E7%99%BC%E7%94%9F-CUDA-%E7%9B%B8%E9%97%9C%E7%9A%84%E7%B7%A8%E8%AD%AF%E9%8C%AF%E8%AA%A4"><span class="nav-number">3.3.1.</span> <span class="nav-text">若發生 CUDA 相關的編譯錯誤</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%BD%89%E6%8F%9B%E7%82%BA-GGUF-%E6%A0%BC%E5%BC%8F"><span class="nav-number">4.</span> <span class="nav-text">模型轉換為 GGUF 格式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BC%89%E5%8E%9F%E5%A7%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">下載原始模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-huggingface-cli-%E4%B8%8B%E8%BC%89"><span class="nav-number">4.1.1.</span> <span class="nav-text">使用 huggingface-cli 下載</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%9D%E8%BD%89%E6%8F%9B%E6%A8%A1%E5%9E%8B%E7%94%A8%E7%9A%84%E7%9B%B8%E4%BE%9D%E6%80%A7%E5%87%BD%E5%BC%8F%E5%BA%AB"><span class="nav-number">4.1.2.</span> <span class="nav-text">安裝轉換模型用的相依性函式庫</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BD%89%E6%8F%9B%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.1.3.</span> <span class="nav-text">轉換模型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96"><span class="nav-number">5.</span> <span class="nav-text">模型量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2-AI-%E6%87%89%E7%94%A8"><span class="nav-number">6.</span> <span class="nav-text">部署 AI 應用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%8F%E9%81%8E-llama-cli-%E9%80%B2%E8%A1%8C%E6%8E%A8%E8%AB%96"><span class="nav-number">6.1.</span> <span class="nav-text">透過 llama-cli 進行推論</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%8F%E9%81%8E-llama-server-%E5%95%9F%E5%8B%95-OpenAI-%E7%9B%B8%E5%AE%B9%E7%9A%84-API-%E4%BC%BA%E6%9C%8D%E5%99%A8"><span class="nav-number">6.2.</span> <span class="nav-text">透過 llama-server 啟動 OpenAI 相容的 API 伺服器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E8%88%87%E4%B9%8B%E4%BA%92%E5%8B%95"><span class="nav-number">6.2.1.</span> <span class="nav-text">如何與之互動</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B5%90%E8%AA%9E"><span class="nav-number">7.</span> <span class="nav-text">結語</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hydai"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">hydai</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分類</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hydai" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hydai" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hydai@hyd.ai" title="E-Mail → mailto:hydai@hyd.ai" rel="noopener me"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://x.com/hydai_tw" title="Twitter → https:&#x2F;&#x2F;x.com&#x2F;hydai_tw" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/hungyingdai" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;hungyingdai" rel="noopener me" target="_blank"><i class="fab fa-facebook fa-fw"></i>FB Page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.youtube.com/@hydai" title="YouTube → https:&#x2F;&#x2F;www.youtube.com&#x2F;@hydai" rel="noopener me" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml" rel="noopener me"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_TW" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="hydai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hydaiの空想世界">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="在 NVIDIA Jetson Orin AGX 上編譯 llama.cpp 與部署 AI 應用 | hydaiの空想世界">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          在 NVIDIA Jetson Orin AGX 上編譯 llama.cpp 與部署 AI 應用
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>
      

      <time title="創建時間：2025-03-07 13:28:00 / 修改時間：15:31:15" itemprop="dateCreated datePublished" datetime="2025-03-07T13:28:00+08:00">2025-03-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分類於</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Note/" itemprop="url" rel="index"><span itemprop="name">Note</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><img src="/2025/03/07/llamacpp-on-jetson-orin-agx/img.jpg" class="">

<h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>由於工作的緣故，需要將 llama.cpp 作為 WASI-NN 的後端所使用，來讓 WebAssembly 能具備使用 AI 模型的能力，也因此需要在各種平台上編譯 llama.cpp 作為我們的相依性函式庫。<br>而 NVIDIA Jetson Orin AGX 64GB 的版本，作為提供相對大的 VRAM 與支援 CUDA 的平台，自然是我們花許多力氣在上面進行測試與最佳化的目標。<br>本文將詳細記錄如何在 NVIDIA Jetson Orin AGX (JetPack 6.2) 上成功編譯 llama.cpp、將大型語言模型轉換成 GGUF 格式、進行模型量化以及最終部署 AI 應用的完整流程。</p>
<span id="more"></span>

<h2 id="升級至-JetPack-6-2"><a href="#升級至-JetPack-6-2" class="headerlink" title="升級至 JetPack 6.2"></a>升級至 JetPack 6.2</h2><p>不論你使用的是 NVIDIA Jetson Orin AGX 或者 Jetson Orin Nano，都強烈建議你升級到 JetPack 6.2 以上的版本，以確保你能夠使用最新的 CUDA 與與解鎖後的效能，同時這個版本提供了 Ubuntu 22.04 與 CUDA 12.6 的環境，具備更好的支援性。<br>至於怎麼安裝，根據不同的型號，操作可能有所差異，請<a target="_blank" rel="noopener" href="https://docs.nvidia.com/jetson/jetpack/install-setup/index.html">參考 NVIDIA 官方文件</a>，以取得最新的安裝方式，這裡不再贅述。</p>
<h2 id="編譯-llama-cpp"><a href="#編譯-llama-cpp" class="headerlink" title="編譯 llama.cpp"></a>編譯 llama.cpp</h2><h3 id="安裝相依性套件"><a href="#安裝相依性套件" class="headerlink" title="安裝相依性套件"></a>安裝相依性套件</h3><p>若升級到 JetPack 6.2 以上，CUDA 12.6 的工具鏈已經預先安裝在系統中，不過還是需要安裝一些相依性套件，以確保編譯 llama.cpp 時不會遇到問題。</p>
<ul>
<li><code>build-essential</code>: 安裝編譯工具鏈，如: GCC&#x2F;G++</li>
<li><code>cmake</code>: 安裝 CMake 編譯工具，目前 llama.cpp 已經改為使用 CMake 進行編譯</li>
<li><code>git</code>: 安裝 Git 版本控制工具，用來下載 llama.cpp 的原始碼</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt update <span class="comment"># 更新套件清單</span></span><br><span class="line"><span class="built_in">sudo</span> apt upgrade -y <span class="comment"># 升級所有已安裝的套件</span></span><br><span class="line"><span class="built_in">sudo</span> apt install -y build-essential cmake git <span class="comment"># 安裝編譯工具鏈、CMake 與 Git</span></span><br></pre></td></tr></table></figure>

<h3 id="下載-llama-cpp"><a href="#下載-llama-cpp" class="headerlink" title="下載 llama.cpp"></a>下載 llama.cpp</h3><p>如果你以前下載過 llama.cpp 的 repo ，他的位置已經從 <code>https://github.com/ggerganov/llama.cpp.git</code> 轉移到 <code>https://github.com/ggml-org/llama.cpp.git</code> 囉，請記得更新你的連結。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ggml-org/llama.cpp.git</span><br><span class="line"><span class="built_in">cd</span> llama.cpp</span><br></pre></td></tr></table></figure>

<h3 id="編譯-llama-cpp-1"><a href="#編譯-llama-cpp-1" class="headerlink" title="編譯 llama.cpp"></a>編譯 llama.cpp</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cmake -B build -DGGML_CUDA=ON <span class="comment"># 在 build 目錄下產生編譯檔案</span></span><br><span class="line">cmake --build build --parallel <span class="comment"># 平行編譯，榨乾該機器上的所有 CPU 資源</span></span><br></pre></td></tr></table></figure>

<p>以我的 Jetson Orin AGX 為例，編譯 llama.cpp 大約需要 30 分鐘左右，視機器效能而定。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake --build build --parallel  9873.06s user 246.00s system 645% cpu 26:07.55 total</span><br></pre></td></tr></table></figure>

<p>編譯完成後，在 <code>build/bin</code> 中會產生 llama.cpp 的所有可執行檔案。</p>
<h4 id="若發生-CUDA-相關的編譯錯誤"><a href="#若發生-CUDA-相關的編譯錯誤" class="headerlink" title="若發生 CUDA 相關的編譯錯誤"></a>若發生 CUDA 相關的編譯錯誤</h4><p>如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">-- Found CUDAToolkit: /usr/local/cuda/include (found version <span class="string">&quot;12.6.68&quot;</span>)</span><br><span class="line">-- CUDA Toolkit found</span><br><span class="line">-- Using CUDA architectures: 50;61;70;75;80</span><br><span class="line">CMake Error at /usr/share/cmake-3.22/Modules/CMakeDetermineCompilerId.cmake:726 (message):</span><br><span class="line">  Compiling the CUDA compiler identification <span class="built_in">source</span> file</span><br><span class="line">  <span class="string">&quot;CMakeCUDACompilerId.cu&quot;</span> failed.</span><br><span class="line"></span><br><span class="line">  Compiler: CMAKE_CUDA_COMPILER-NOTFOUND</span><br><span class="line"></span><br><span class="line">  Build flags:</span><br><span class="line"></span><br><span class="line">  Id flags: -v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  The output was:</span><br><span class="line"></span><br><span class="line">  No such file or directory</span><br><span class="line"></span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  /usr/share/cmake-3.22/Modules/CMakeDetermineCompilerId.cmake:6 (CMAKE_DETERMINE_COMPILER_ID_BUILD)</span><br><span class="line">  /usr/share/cmake-3.22/Modules/CMakeDetermineCompilerId.cmake:48 (__determine_compiler_id_test)</span><br><span class="line">  /usr/share/cmake-3.22/Modules/CMakeDetermineCUDACompiler.cmake:298 (CMAKE_DETERMINE_COMPILER_ID)</span><br><span class="line">  ggml/src/ggml-cuda/CMakeLists.txt:25 (enable_language)</span><br><span class="line"></span><br><span class="line">-- Configuring incomplete, errors occurred!</span><br></pre></td></tr></table></figure>

<p>這是因為 CUDA 12.6 的編譯器路徑不正確，需要手動設定 CUDA 編譯器路徑，請執行以下指令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda-12.6/bin<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-12.6/lib64\</span><br><span class="line">                         <span class="variable">$&#123;LD_LIBRARY_PATH:+:<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>&#125;</span></span><br></pre></td></tr></table></figure>

<p>接著重新執行上述的編譯指令即可解決。</p>
<h2 id="模型轉換為-GGUF-格式"><a href="#模型轉換為-GGUF-格式" class="headerlink" title="模型轉換為 GGUF 格式"></a>模型轉換為 GGUF 格式</h2><p>多數的模型並不是以 GGUF 的格式進行發布的，因此在使用 llama.cpp 執行之前，需要先將模型轉換成 GGUF 格式才能夠正確載入。<br>當然在 Hugging Face 上已經有不少人轉換好的 GGUF 模型，如果你是下載那些已經轉換的檔案就可以跳過此步驟。</p>
<h3 id="下載原始模型"><a href="#下載原始模型" class="headerlink" title="下載原始模型"></a>下載原始模型</h3><p>此處以 <a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B">DeepSeek-R1-Distill-Llama-8B</a> 為例，你可以換成任何你喜歡的模型。</p>
<h4 id="使用-huggingface-cli-下載"><a href="#使用-huggingface-cli-下載" class="headerlink" title="使用 huggingface-cli 下載"></a>使用 huggingface-cli 下載</h4><p>由於下載需要不少時間，建議使用 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/huggingface_hub/en/guides/cli">huggingface-cli</a> 來下載模型。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli download deepseek-ai/DeepSeek-R1-Distill-Llama-8B --local-dir ds-r1-distill-llama-8b</span><br><span class="line"><span class="comment"># huggingface-cli download &lt;org/repo&gt; --local-dir &lt;存放在本機端的路徑&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="安裝轉換模型用的相依性函式庫"><a href="#安裝轉換模型用的相依性函式庫" class="headerlink" title="安裝轉換模型用的相依性函式庫"></a>安裝轉換模型用的相依性函式庫</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 llama.cpp 的根目錄下執行</span></span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure>

<h4 id="轉換模型"><a href="#轉換模型" class="headerlink" title="轉換模型"></a>轉換模型</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python convert_hf_to_gguf.py --outfile ./ds-r1-distill-llama-8b ./ds-r1-distill-llama-8b</span><br><span class="line"><span class="comment"># python convert_hf_to_gguf.py --outfile &lt;輸出檔案資料夾&gt; &lt;模型所在資料夾&gt;</span></span><br></pre></td></tr></table></figure>

<p>以下為執行的 log 僅供參考：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">INFO:hf-to-gguf:Loading model: ds-r1-distill-llama-8b</span><br><span class="line">INFO:gguf.gguf_writer:gguf: This GGUF file is <span class="keyword">for</span> Little Endian only</span><br><span class="line">INFO:hf-to-gguf:Exporting model...</span><br><span class="line">INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --&gt; F32, shape = &#123;64&#125;</span><br><span class="line">INFO:hf-to-gguf:gguf: loading model weight map from <span class="string">&#x27;model.safetensors.index.json&#x27;</span></span><br><span class="line">INFO:hf-to-gguf:gguf: loading model part <span class="string">&#x27;model-00001-of-000002.safetensors&#x27;</span></span><br><span class="line">INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --&gt; F16, shape = &#123;4096, 128256&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --&gt; F32, shape = &#123;4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --&gt; F16, shape = &#123;14336, 4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --&gt; F16, shape = &#123;4096, 14336&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --&gt; F16, shape = &#123;4096, 14336&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --&gt; F32, shape = &#123;4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --&gt; F16, shape = &#123;4096, 1024&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --&gt; F16, shape = &#123;4096, 4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --&gt; F16, shape = &#123;4096, 4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --&gt; F16, shape = &#123;4096, 1024&#125;</span><br><span class="line">...中間省略...</span><br><span class="line">INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --&gt; F32, shape = &#123;4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --&gt; F16, shape = &#123;14336, 4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --&gt; F16, shape = &#123;4096, 14336&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --&gt; F16, shape = &#123;4096, 14336&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --&gt; F32, shape = &#123;4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --&gt; F16, shape = &#123;4096, 1024&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --&gt; F16, shape = &#123;4096, 4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --&gt; F16, shape = &#123;4096, 4096&#125;</span><br><span class="line">INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --&gt; F16, shape = &#123;4096, 1024&#125;</span><br><span class="line">INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --&gt; F32, shape = &#123;4096&#125;</span><br><span class="line">INFO:hf-to-gguf:Set meta model</span><br><span class="line">INFO:hf-to-gguf:Set model parameters</span><br><span class="line">INFO:hf-to-gguf:gguf: context length = 131072</span><br><span class="line">INFO:hf-to-gguf:gguf: embedding length = 4096</span><br><span class="line">INFO:hf-to-gguf:gguf: feed forward length = 14336</span><br><span class="line">INFO:hf-to-gguf:gguf: <span class="built_in">head</span> count = 32</span><br><span class="line">INFO:hf-to-gguf:gguf: key-value <span class="built_in">head</span> count = 8</span><br><span class="line">INFO:hf-to-gguf:gguf: rope theta = 500000.0</span><br><span class="line">INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05</span><br><span class="line">INFO:hf-to-gguf:gguf: file <span class="built_in">type</span> = 1</span><br><span class="line">INFO:hf-to-gguf:Set model tokenizer</span><br><span class="line">INFO:gguf.vocab:Adding 280147 merge(s).</span><br><span class="line">INFO:gguf.vocab:Setting special token <span class="built_in">type</span> bos to 128000</span><br><span class="line">INFO:gguf.vocab:Setting special token <span class="built_in">type</span> eos to 128001</span><br><span class="line">INFO:gguf.vocab:Setting special token <span class="built_in">type</span> pad to 128001</span><br><span class="line">INFO:gguf.vocab:Setting add_bos_token to True</span><br><span class="line">INFO:gguf.vocab:Setting add_eos_token to False</span><br><span class="line">INFO:gguf.vocab:Setting chat_template to &#123;% <span class="keyword">if</span> not add_generation_prompt is defined %&#125;&#123;% <span class="built_in">set</span> add_generation_prompt = <span class="literal">false</span> %&#125;&#123;% endif %&#125;&#123;% <span class="built_in">set</span> ns = namespace(is_first=<span class="literal">false</span>, is_tool=<span class="literal">false</span>, is_output_first=<span class="literal">true</span>, system_prompt=<span class="string">&#x27;&#x27;</span>) %&#125;&#123;%- <span class="keyword">for</span> message <span class="keyword">in</span> messages %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;system&#x27;</span> %&#125;&#123;% <span class="built_in">set</span> ns.system_prompt = message[<span class="string">&#x27;content&#x27;</span>] %&#125;&#123;%- endif %&#125;&#123;%- endfor %&#125;&#123;&#123;bos_token&#125;&#125;&#123;&#123;ns.system_prompt&#125;&#125;&#123;%- <span class="keyword">for</span> message <span class="keyword">in</span> messages %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;user&#x27;</span> %&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = <span class="literal">false</span> -%&#125;&#123;&#123;<span class="string">&#x27;&lt;｜User｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>]&#125;&#125;&#123;%- endif %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;assistant&#x27;</span> and message[<span class="string">&#x27;content&#x27;</span>] is none %&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = <span class="literal">false</span> -%&#125;&#123;%- <span class="keyword">for</span> tool <span class="keyword">in</span> message[<span class="string">&#x27;tool_calls&#x27;</span>]%&#125;&#123;%- <span class="keyword">if</span> not ns.is_first %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜Assistant｜&gt;&lt;｜tool▁calls▁begin｜&gt;&lt;｜tool▁call▁begin｜&gt;&#x27;</span> + tool[<span class="string">&#x27;type&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁sep｜&gt;&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```json&#x27;</span> + <span class="string">&#x27;\n&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;arguments&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```&#x27;</span> + <span class="string">&#x27;&lt;｜tool▁call▁end｜&gt;&#x27;</span>&#125;&#125;&#123;%- <span class="built_in">set</span> ns.is_first = <span class="literal">true</span> -%&#125;&#123;%- <span class="keyword">else</span> %&#125;&#123;&#123;<span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;&lt;｜tool▁call▁begin｜&gt;&#x27;</span> + tool[<span class="string">&#x27;type&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁sep｜&gt;&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;name&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```json&#x27;</span> + <span class="string">&#x27;\n&#x27;</span> + tool[<span class="string">&#x27;function&#x27;</span>][<span class="string">&#x27;arguments&#x27;</span>] + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;```&#x27;</span> + <span class="string">&#x27;&lt;｜tool▁call▁end｜&gt;&#x27;</span>&#125;&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁calls▁end｜&gt;&lt;｜end▁of▁sentence｜&gt;&#x27;</span>&#125;&#125;&#123;%- endif %&#125;&#123;%- endfor %&#125;&#123;%- endif %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;assistant&#x27;</span> and message[<span class="string">&#x27;content&#x27;</span>] is not none %&#125;&#123;%- <span class="keyword">if</span> ns.is_tool %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁outputs▁end｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>] + <span class="string">&#x27;&lt;｜end▁of▁sentence｜&gt;&#x27;</span>&#125;&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = <span class="literal">false</span> -%&#125;&#123;%- <span class="keyword">else</span> %&#125;&#123;% <span class="built_in">set</span> content = message[<span class="string">&#x27;content&#x27;</span>] %&#125;&#123;% <span class="keyword">if</span> <span class="string">&#x27;&lt;/think&gt;&#x27;</span> <span class="keyword">in</span> content %&#125;&#123;% <span class="built_in">set</span> content = content.split(<span class="string">&#x27;&lt;/think&gt;&#x27;</span>)[-1] %&#125;&#123;% endif %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜Assistant｜&gt;&#x27;</span> + content + <span class="string">&#x27;&lt;｜end▁of▁sentence｜&gt;&#x27;</span>&#125;&#125;&#123;%- endif %&#125;&#123;%- endif %&#125;&#123;%- <span class="keyword">if</span> message[<span class="string">&#x27;role&#x27;</span>] == <span class="string">&#x27;tool&#x27;</span> %&#125;&#123;%- <span class="built_in">set</span> ns.is_tool = <span class="literal">true</span> -%&#125;&#123;%- <span class="keyword">if</span> ns.is_output_first %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁outputs▁begin｜&gt;&lt;｜tool▁output▁begin｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁output▁end｜&gt;&#x27;</span>&#125;&#125;&#123;%- <span class="built_in">set</span> ns.is_output_first = <span class="literal">false</span> %&#125;&#123;%- <span class="keyword">else</span> %&#125;&#123;&#123;<span class="string">&#x27;\n&lt;｜tool▁output▁begin｜&gt;&#x27;</span> + message[<span class="string">&#x27;content&#x27;</span>] + <span class="string">&#x27;&lt;｜tool▁output▁end｜&gt;&#x27;</span>&#125;&#125;&#123;%- endif %&#125;&#123;%- endif %&#125;&#123;%- endfor -%&#125;&#123;% <span class="keyword">if</span> ns.is_tool %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜tool▁outputs▁end｜&gt;&#x27;</span>&#125;&#125;&#123;% endif %&#125;&#123;% <span class="keyword">if</span> add_generation_prompt and not ns.is_tool %&#125;&#123;&#123;<span class="string">&#x27;&lt;｜Assistant｜&gt;&lt;think&gt;\n&#x27;</span>&#125;&#125;&#123;% endif %&#125;</span><br><span class="line">INFO:hf-to-gguf:Set model quantization version</span><br><span class="line">INFO:gguf.gguf_writer:Writing the following files:</span><br><span class="line">INFO:gguf.gguf_writer:/disk/models/ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-F16.gguf: n_tensors = 292, total_size = 16.1G</span><br><span class="line">Writing: 100%|███████████████████████████████████████████████████████████████████████████████████| 16.1G/16.1G [01:14&lt;00:00, 216Mbyte/s]</span><br><span class="line">INFO:hf-to-gguf:Model successfully exported to /disk/models/ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-F16.gguf</span><br></pre></td></tr></table></figure>

<p>在轉換後我們可以初步看到模型的大小關係，基本上沒有太大的變化：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 轉換前的模型大小 (8.1G + 6.9G = 15G)</span><br><span class="line">8.1G model-00001-of-000002.safetensors</span><br><span class="line">6.9G model-00002-of-000002.safetensors</span><br><span class="line"># 轉換後的 F16 GGUF</span><br><span class="line">15G ds-r1-distill-llama-8B-F16.gguf</span><br></pre></td></tr></table></figure>

<h2 id="模型量化"><a href="#模型量化" class="headerlink" title="模型量化"></a>模型量化</h2><p>由於眾所周知的原因（大家都很窮），部署完整版(F16)的多數模型實在過於龐大，除了因為大小的緣故很難被放入消費級的硬體外，還容易因為硬體效能限制導致推理速度不理想。這時候我們就會需要透過量化，雖然降低精度，卻也能保證模型被縮小到可以較容易部署的大小，並能獲得更高的推理速度，達到更好的可用性。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 llama.cpp 的根目錄下執行</span></span><br><span class="line">./build/bin/llama-quantize ./ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-F16.gguf ./ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-Q4_0.gguf q4_0</span><br><span class="line"><span class="comment"># ./build/bin/llama-quantize &lt;f16 gguf 檔案路徑&gt; &lt;量化後的 gguf 檔案路徑&gt; &lt;量化的方法&gt;</span></span><br></pre></td></tr></table></figure>

<p>僅供參考的 log:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">main: build = 4845 (d6c95b07)</span><br><span class="line">main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for aarch64-linux-gnu</span><br><span class="line">main: quantizing &#x27;/disk/models/ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-F16.gguf&#x27; to &#x27;/disk/models/ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-Q4_0.gguf&#x27; as Q4_0</span><br><span class="line">llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /disk/models/ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-F16.gguf (version GGUF V3 (latest))</span><br><span class="line">llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.</span><br><span class="line">llama_model_loader: - kv   0:                       general.architecture str              = llama</span><br><span class="line">llama_model_loader: - kv   1:                               general.type str              = model</span><br><span class="line">llama_model_loader: - kv   2:                               general.name str              = Ds R1 Distill Llama 8b</span><br><span class="line">llama_model_loader: - kv   3:                           general.basename str              = ds-r1-distill-llama</span><br><span class="line">llama_model_loader: - kv   4:                         general.size_label str              = 8B</span><br><span class="line">llama_model_loader: - kv   5:                            general.license str              = mit</span><br><span class="line">llama_model_loader: - kv   6:                          llama.block_count u32              = 32</span><br><span class="line">llama_model_loader: - kv   7:                       llama.context_length u32              = 131072</span><br><span class="line">llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096</span><br><span class="line">llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336</span><br><span class="line">llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32</span><br><span class="line">llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8</span><br><span class="line">llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000</span><br><span class="line">llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010</span><br><span class="line">llama_model_loader: - kv  14:                          general.file_type u32              = 1</span><br><span class="line">llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256</span><br><span class="line">llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128</span><br><span class="line">llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2</span><br><span class="line">llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe</span><br><span class="line">llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [&quot;!&quot;, &quot;\&quot;&quot;, &quot;#&quot;, &quot;$&quot;, &quot;%&quot;, &quot;&amp;&quot;, &quot;&#x27;&quot;, ...</span><br><span class="line">llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</span><br><span class="line">llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [&quot;Ġ Ġ&quot;, &quot;Ġ ĠĠĠ&quot;, &quot;ĠĠ ĠĠ&quot;, &quot;...</span><br><span class="line">llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000</span><br><span class="line">llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128001</span><br><span class="line">llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128001</span><br><span class="line">llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true</span><br><span class="line">llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = false</span><br><span class="line">llama_model_loader: - kv  27:                    tokenizer.chat_template str              = &#123;% if not add_generation_prompt is de...</span><br><span class="line">llama_model_loader: - kv  28:               general.quantization_version u32              = 2</span><br><span class="line">llama_model_loader: - type  f32:   66 tensors</span><br><span class="line">llama_model_loader: - type  f16:  226 tensors</span><br><span class="line">ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no</span><br><span class="line">ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no</span><br><span class="line">ggml_cuda_init: found 1 CUDA devices:</span><br><span class="line">  Device 0: Orin, compute capability 8.7, VMM: yes</span><br><span class="line">[   1/ 292]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB -&gt;   410.98 MiB</span><br><span class="line">[   2/ 292]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB</span><br><span class="line">[   3/ 292]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB</span><br><span class="line">[   4/ 292]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q4_0 .. size =  1002.00 MiB -&gt;   281.81 MiB</span><br><span class="line">[   5/ 292]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB -&gt;     2.25 MiB</span><br><span class="line">[   6/ 292]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB</span><br><span class="line">[   7/ 292]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB -&gt;     9.00 MiB</span><br><span class="line">[   8/ 292]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB -&gt;     9.00 MiB</span><br><span class="line">[   9/ 292]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB -&gt;     2.25 MiB</span><br><span class="line">[  10/ 292]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_0 .. size =   112.00 MiB -&gt;    31.50 MiB</span><br><span class="line">[  11/ 292]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_0 .. size =   112.00 MiB -&gt;    31.50 MiB</span><br><span class="line">[  12/ 292]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB</span><br><span class="line">[  13/ 292]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_0 .. size =   112.00 MiB -&gt;    31.50 MiB</span><br><span class="line">...中間省略...</span><br><span class="line">[ 284/ 292]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB -&gt;     2.25 MiB</span><br><span class="line">[ 285/ 292]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB</span><br><span class="line">[ 286/ 292]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB -&gt;     9.00 MiB</span><br><span class="line">[ 287/ 292]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_0 .. size =    32.00 MiB -&gt;     9.00 MiB</span><br><span class="line">[ 288/ 292]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_0 .. size =     8.00 MiB -&gt;     2.25 MiB</span><br><span class="line">[ 289/ 292]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_0 .. size =   112.00 MiB -&gt;    31.50 MiB</span><br><span class="line">[ 290/ 292]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_0 .. size =   112.00 MiB -&gt;    31.50 MiB</span><br><span class="line">[ 291/ 292]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB</span><br><span class="line">[ 292/ 292]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_0 .. size =   112.00 MiB -&gt;    31.50 MiB</span><br><span class="line">llama_model_quantize_impl: model size  = 15317.02 MB</span><br><span class="line">llama_model_quantize_impl: quant size  =  4437.80 MB</span><br><span class="line"></span><br><span class="line">main: quantize time = 17884.06 ms</span><br><span class="line">main:    total time = 17884.06 ms</span><br></pre></td></tr></table></figure>

<p>在進行完量化後，模型大小差異如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">原本為 15GB，量化後為 4.4GB</span><br><span class="line">llama_model_quantize_impl: model size  = 15317.02 MB</span><br><span class="line">llama_model_quantize_impl: quant size  =  4437.80 MB</span><br></pre></td></tr></table></figure>

<h2 id="部署-AI-應用"><a href="#部署-AI-應用" class="headerlink" title="部署 AI 應用"></a>部署 AI 應用</h2><h3 id="透過-llama-cli-進行推論"><a href="#透過-llama-cli-進行推論" class="headerlink" title="透過 llama-cli 進行推論"></a>透過 llama-cli 進行推論</h3><p>直接使用 Chat CLI 來與模型進行互動，透過以下指令來進行推論：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/bin/llama-cli -m ./ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-Q4_0.gguf</span><br></pre></td></tr></table></figure>

<p>然後你會馬上發現，似乎 GPU 沒有被使用到，那就對了，這時會在 log 中發現以下資訊：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">load_tensors: loading model tensors, this can take a while... (mmap = true)</span><br><span class="line">load_tensors: offloading 0 repeating layers to GPU</span><br><span class="line">load_tensors: offloaded 0/33 layers to GPU</span><br><span class="line">load_tensors:  CPU_AARCH64 model buffer size =  3744.00 MiB</span><br><span class="line">load_tensors:   CPU_Mapped model buffer size =  4406.30 MiB</span><br></pre></td></tr></table></figure>

<p>告訴你他只使用了 CPU 的部分，並沒有將模型放到 GPU 上來進行加速。</p>
<p>你需要指定 <code>-ngl 層數</code> 來指定要放到 GPU 上的層數，例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/bin/llama-cli -m ./ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-Q4_0.gguf -ngl 33</span><br></pre></td></tr></table></figure>

<p>這時就會看見 GPU 被使用到了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">load_tensors: loading model tensors, this can take a while... (mmap = true)</span><br><span class="line">load_tensors: offloading 32 repeating layers to GPU</span><br><span class="line">load_tensors: offloading output layer to GPU</span><br><span class="line">load_tensors: offloaded 33/33 layers to GPU</span><br><span class="line">load_tensors:        CUDA0 model buffer size =  4155.99 MiB</span><br><span class="line">load_tensors:   CPU_Mapped model buffer size =   281.81 MiB</span><br></pre></td></tr></table></figure>

<h3 id="透過-llama-server-啟動-OpenAI-相容的-API-伺服器"><a href="#透過-llama-server-啟動-OpenAI-相容的-API-伺服器" class="headerlink" title="透過 llama-server 啟動 OpenAI 相容的 API 伺服器"></a>透過 llama-server 啟動 OpenAI 相容的 API 伺服器</h3><p>透過以下指令來啟動 llama-server：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build/bin/llama-cli -m ./ds-r1-distill-llama-8b/ds-r1-distill-llama-8B-Q4_0.gguf -ngl 33</span><br></pre></td></tr></table></figure>

<p>原則上就是把上面的 <code>cli</code> 換成 <code>server</code> 就可以囉！</p>
<h4 id="如何與之互動"><a href="#如何與之互動" class="headerlink" title="如何與之互動"></a>如何與之互動</h4><p>請查詢 OpenAI API 該如何使用，你可以直接將任何支援 OpenAI API endpoint 的工具與服務換成 llama-server 啟動的伺服器。</p>
<p>以下只是示範如何使用 <code>curl</code> 來發送請求，並使用 <code>jq</code> 來渲染輸出結果</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8080/v1/chat/completions \</span><br><span class="line">    -H <span class="string">&#x27;accept:application/json&#x27;</span> \</span><br><span class="line">    -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">    -d <span class="string">&#x27;&#123;&quot;messages&quot;:[&#123;&quot;role&quot;:&quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant. Reply questions in less than five words.&quot;&#125;, &#123;&quot;role&quot;:&quot;user&quot;, &quot;content&quot;: &quot;What is the capital of Japan?&quot;&#125;], &quot;model&quot;:&quot;default&quot;&#125;&#x27;</span> | jq .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 輸出結果</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;choices&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">&quot;finish_reason&quot;</span>: <span class="string">&quot;stop&quot;</span>,</span><br><span class="line">      <span class="string">&quot;index&quot;</span>: 0,</span><br><span class="line">      <span class="string">&quot;message&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;&lt;think&gt;\nOkay, so I need to figure out the capital of Japan. Hmm, I&#x27;m not entirely sure, but I think it&#x27;s a country that&#x27;s pretty well-known, so maybe I can recall it from memory. Let me try to think. I remember that Tokyo is a major city there. Wait, is Tokyo the capital? Or is it another city? I think I&#x27;ve heard that Tokyo is both the capital and the largest city. I&#x27;m pretty sure other countries have capitals that are also their biggest cities, like Washington D.C. in the U.S. So applying that logic, Japan&#x27;s capital should be Tokyo. I don&#x27;t think it&#x27;s Osaka or Nagasaki because those are other major cities, but I&#x27;m almost certain the capital is Tokyo. Yeah, I feel confident about that.\n&lt;/think&gt;\n\nTokyo&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;created&quot;</span>: 1741332025,</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;default&quot;</span>,</span><br><span class="line">  <span class="string">&quot;system_fingerprint&quot;</span>: <span class="string">&quot;b4845-d6c95b07&quot;</span>,</span><br><span class="line">  <span class="string">&quot;object&quot;</span>: <span class="string">&quot;chat.completion&quot;</span>,</span><br><span class="line">  <span class="string">&quot;usage&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;completion_tokens&quot;</span>: 167,</span><br><span class="line">    <span class="string">&quot;prompt_tokens&quot;</span>: 24,</span><br><span class="line">    <span class="string">&quot;total_tokens&quot;</span>: 191</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;id&quot;</span>: <span class="string">&quot;chatcmpl-cRBfkobRrFxvmeQxxL6ZYipDB60TSmQr&quot;</span>,</span><br><span class="line">  <span class="string">&quot;timings&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;prompt_n&quot;</span>: 13,</span><br><span class="line">    <span class="string">&quot;prompt_ms&quot;</span>: 806.983,</span><br><span class="line">    <span class="string">&quot;prompt_per_token_ms&quot;</span>: 62.07561538461538,</span><br><span class="line">    <span class="string">&quot;prompt_per_second&quot;</span>: 16.109385203901446,</span><br><span class="line">    <span class="string">&quot;predicted_n&quot;</span>: 167,</span><br><span class="line">    <span class="string">&quot;predicted_ms&quot;</span>: 17762.902,</span><br><span class="line">    <span class="string">&quot;predicted_per_token_ms&quot;</span>: 106.36468263473053,</span><br><span class="line">    <span class="string">&quot;predicted_per_second&quot;</span>: 9.401616920478423</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h2><p>本文以 <code>llama.cpp b4845 (d6c95b07)</code> 做教學，由於 llama.cpp 是非常活躍的專案，因此上述的工具參數可能有所修改，如發現該指令無法正確執行，請使用 <code>--help</code> 或者 <code>-h</code> 來查看最新的參數應該如何使用。<br>希望本文能讓想使用 NVIDIA Jetson 系列開發版的開發者們，能夠輕鬆上手 llama.cpp，並更快速地部署 AI 應用。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>hydai
  </li>
  <li class="post-copyright-link">
      <strong>文章連結：</strong>
      <a href="https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/" title="在 NVIDIA Jetson Orin AGX 上編譯 llama.cpp 與部署 AI 應用">https://hyd.ai/2025/03/07/llamacpp-on-jetson-orin-agx/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版權聲明： </strong>本網誌所有文章除特別聲明外，均採用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh_TW" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 許可協議。轉載請註明出處！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Linux/" rel="tag"># Linux</a>
              <a href="/tags/LLM/" rel="tag"># LLM</a>
              <a href="/tags/NVIDIA/" rel="tag"># NVIDIA</a>
              <a href="/tags/Jetson/" rel="tag"># Jetson</a>
              <a href="/tags/Orin/" rel="tag"># Orin</a>
              <a href="/tags/AGX/" rel="tag"># AGX</a>
              <a href="/tags/JetPack/" rel="tag"># JetPack</a>
              <a href="/tags/CUDA/" rel="tag"># CUDA</a>
              <a href="/tags/GGUF/" rel="tag"># GGUF</a>
              <a href="/tags/llamacpp/" rel="tag"># llamacpp</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/12/12/deploy-flatcar-on-do/" rel="prev" title="在 DigitalOcean 上部署 Flatcar Container Linux + LLM">
                  <i class="fa fa-angle-left"></i> 在 DigitalOcean 上部署 Flatcar Container Linux + LLM
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2016 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">hydai</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 強力驅動
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="回到頂端">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"hydai/hydai.github.io","issue_term":"pathname","theme":"github-dark","pathname":"pathname","cdn":"https://utteranc.es/client.js","label":"✨💬✨Comment"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
